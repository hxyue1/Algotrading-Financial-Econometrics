---
title: "Algotrading"
author: "Hong Xiang Yue"
date: "01/05/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Importing data and EDA
Importing data from csv. 

Vanguard ETFS:
- VAF
- VAP
- VAS
- VGS

```{r}
setwd("~/Desktop/Documents/Self study/Quant Finance/Algotrading/ETFS/Data/Vanguard/")
vaf <- read.csv("VAF.AX.csv")
vap <- read.csv("VAP.AX.csv")
vas <- read.csv("VAS.AX.csv")
vgs <- read.csv("VGS.AX.csv")


```
#Modelling returns

Deleting rows with null entries

```{r}
vaf <- vaf[!(vaf$Open=="null"),]
vap <- vap[!(vap$Open=="null"),]
vas <- vas[!(vas$Open=="null"),]
vgs <- vgs[!(vgs$Open=="null"),]

vaf$Close <- as.numeric(levels(vaf$Close))[vaf$Close]
vap$Close <- as.numeric(levels(vap$Close))[vap$Close]
vas$Close <- as.numeric(levels(vas$Close))[vas$Close]
vgs$Close <- as.numeric(levels(vgs$Close))[vgs$Close]
```

Calculating log returns

```{r}
lreturns <- function(df){
  close_prev <- df$Close[1:(nrow(df)-1)]
  close_today <- df$Close[2:nrow(df)]
  lret <- log(close_today/close_prev)
  lret <- c(NA,lret)
  df$LogReturn <- lret
  return(df)
}

vaf <- lreturns(vaf)
vap <- lreturns(vap)
vas <- lreturns(vas)
vgs <- lreturns(vgs)
```

Testing for autocorrelation in returns

```{r}
vaf_acf<-acf(vaf$LogReturn[2:nrow(vaf)])
vap_acf<-acf(vap$LogReturn[2:nrow(vap)])
vas_acf<-acf(vas$LogReturn[2:nrow(vas)])
vgs_acf<-acf(vgs$LogReturn[2:nrow(vgs)])

Box.test(vaf$LogReturn[2:nrow(vaf)],type="Ljung",lag=10)
Box.test(vap$LogReturn[2:nrow(vap)],type="Ljung",lag=10)
Box.test(vas$LogReturn[2:nrow(vas)],type="Ljung",lag=10)
Box.test(vgs$LogReturn[2:nrow(vgs)],type="Ljung",lag=10)
```

Strong autocorrelation in vaf and vap up to the 10th lag. Borderline statistically significant autocorrelation for vas and vgs.

Fitting AIRMAs to the returns

```{r}
library(forecast)
vaf_arima <-auto.arima(vaf$LogReturn[2:nrow(vaf)])
vap_arima <-auto.arima(vap$LogReturn[2:nrow(vap)])
vas_arima <-auto.arima(vap$LogReturn[2:nrow(vas)])
vgs_arima <-auto.arima(vap$LogReturn[2:nrow(vgs)])
```

Setting up functions for rolling forecasts using ARs and ARIMAs
```{r}

arima_roll_forecast <- function(dependent,train_length,roll_length,arima_dim){
  predictions <- data.frame()
  for (i in 1:roll_length){
    train = dependent[i:(train_length+i-1)]
    #train_arima <- arima(train, order=arima_dim, transform.pars = TRUE, method="ML")
    #train_arima <- arima(train, order=arima_dim, method="ML",optim.control = list(maxit = 2000), optim.method="Nelder-Mead")
    train_arima <- auto.arima(train)
    onestepf <- predict(train_arima,n.ahead=1)
    pred <- onestepf$pred[1]
    predictions <- rbind(predictions,pred)
  }
  
  colnames(predictions) <- ""
  return(predictions)
}

ar_roll_forecast <- function(dependent,p,train_length,roll_length){
  predictions <- data.frame()
  for (i in 1:roll_length){
    train = dependent[i:(train_length+i-1)]
    train_ar <- ar(train, order.max=p)
    onestepf <- predict(train_ar,n.ahead=1)
    pred <- onestepf$pred[1]
    predictions <- rbind(predictions,pred)
  }
  
  colnames(predictions) <- ""
  return(predictions)
}
```


Finding optimal training length by looking at RMSE from rolling forecasts. 

```{r}
dependent = vaf$LogReturn[2:nrow(vaf)]
vaf_RMSE <- data.frame()
for (i in seq(100,1500,20)){
  train_length <- i 
  vaf_ar_preds <- ar_roll_forecast(dependent,1,train_length,length(dependent)-train_length)
  vaf_y <- data.frame(cbind(dependent[train_length:(length(dependent)-1)],vaf_ar_preds))
  colnames(vaf_y) <- c('y','AR')
  vaf_ar_RMSE <- mean((vaf_y$y-vaf_y$AR)^2)^0.5
  vaf_RMSE <- rbind(vaf_RMSE,vaf_ar_RMSE)
}
```
Optimal training length is approximately 1200 observations. It may be that more observations would improve rolling forecast RMSE, but we simply "run out" of test observations to evaluate performance on so the actual calculation can be inaccurate. ARIMA rolling forecasts were removed because of optimisation issues. Minimum RMSE is 0.00183, which is lower than the standard deviation of returns 0.00259 suggesting that there is forecastability in the returns ergo, the EMH does not hold for Australian fixed income assets (or at least derivatives comprising of them).

Doing the same for the vap returns

```{r}
dependent <- vap$LogReturn[2:nrow(vap)]
train_length <- 100
vap_arima_preds <- arima_roll_forecast(dependent,train_length,length(dependent)-train_length,c(2,0,1))
vap_ar_preds <- ar_roll_forecast(dependent,train_length,length(dependent)-train_length)

vap_y <- data.frame(cbind(dependent[train_length:(length(dependent)-1)],vap_arima_preds,vap_ar_preds))
colnames(vap_y) <- c('y','ARIMA','AR')
vap_arima_RMSE <- mean((vap_y$y-vap_y$ARIMA)^2)^0.5
vap_ar_RMSE <- mean((vap_y$y-vap_y$AR)^2)^0.5
print(vap_arima_RMSE)
print(vap_ar_RMSE)
```

```{r}
dependent <- vap$LogReturn[2:nrow(vap)]
vap_RMSE <- data.frame()
for (i in seq(100,1900,100)){
  train_length <- i 
  vap_ar_preds <- ar_roll_forecast(dependent,train_length,length(dependent)-train_length)
  vap_y <- data.frame(cbind(dependent[train_length:(length(dependent)-1)],vap_ar_preds))
  colnames(vap_y) <- c('y','AR')
  vap_ar_RMSE <- mean((vap_y$y-vap_y$AR)^2)^0.5
  vap_RMSE <- rbind(vap_RMSE,vap_ar_RMSE)
}
```

Optimal training length is at approximately 1600 observations for a one-step ahead RMSE of 0.00963 which is still higher than the overall standard deviation of returns 0.0095 unfortunately. This suggests that there is very little predictability at all and that the returns may have a constant mean (EMH).

```{r}
dependent <- vas$LogReturn[2:nrow(vas)]
vas_RMSE <- data.frame()
for (i in seq(100,2300,100)){
  train_length <- i 
  vas_ar_preds <- ar_roll_forecast(dependent,train_length,length(dependent)-train_length)
  vas_y <- data.frame(cbind(dependent[train_length:(length(dependent)-1)],vas_ar_preds))
  colnames(vas_y) <- c('y','AR')
  vas_ar_RMSE <- mean((vas_y$y-vas_y$AR)^2)^0.5
  vas_RMSE <- rbind(vas_RMSE,vas_ar_RMSE)
}
```

Lowest one-step ahead forecast RMSE is 0.0068 (training length 1900 observations) suggesting that there is predictability in asset returns because the unconditional standard deviation is 0.0091. We can noticeably outperform a constant return model.

```{r}
dependent <- vgs$LogReturn[2:nrow(vgs)]
vgs_RMSE <- data.frame()
for (i in seq(490,520,1)){
  train_length <- i 
  vgs_ar_preds <- ar_roll_forecast(dependent,train_length,length(dependent)-train_length)
  vgs_y <- data.frame(cbind(dependent[train_length:(length(dependent)-1)],vgs_ar_preds))
  colnames(vgs_y) <- c('y','AR')
  vgs_ar_RMSE <- mean((vgs_y$y-vgs_y$AR)^2)^0.5
  vgs_RMSE <- rbind(vgs_RMSE,vgs_ar_RMSE)
}
```

Lowest one-step ahead forecast RMSE is 0.0079 which is slightly lower than the unconditional standard deviation of 0.0083. However, for most training lengths, it seems to be above this value suggesting that the improved performance was probably some sort of fluke.

#Modelling volatility

##VAF

```{r}
library(rugarch)

vaf_returns <- vaf$LogReturn[2:nrow(vaf)]
daily_sd <- sqrt((vaf_returns-mean(vaf_returns))^2)
vaf_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"))

vaf_gjrgarch_fit <-ugarchfit(vaf_returns,spec=vaf_gjrgarch_spec)
vaf_std_resids <-residuals(vaf_gjrgarch_fit)/vaf_gjrgarch_fit@fit[["sigma"]]
```

Running diagnostics to check for normality. 
```{r}

vaf_std_resids <- data.frame(vaf_std_resids)
rownames(vaf_std_resids) <- c()

hist(vaf_std_resids$vaf_std_resids)
qqnorm(vaf_std_resids$vaf_std_resids,ylim=c(-6,6),xlim=c(-6,6))
qqline(vaf_std_resids$vaf_std_resids, col = "steelblue", lwd = 2)


shapiro.test(vaf_std_resids$vaf_std_resids)

```

Histogram looks ok, but QQ plot shows deficiencies, particularly in the lower tail. Large price drops are more frequent than predicted according to the model. Shapiro-Wilk test strongly rejects hypothesis of normality. Trying student's-t distribution.


```{r}
vaf_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"), distribution.model="std")

vaf_gjrgarch_fit <-ugarchfit(vaf_returns,spec=vaf_gjrgarch_spec)
vaf_std_resids <-residuals(vaf_gjrgarch_fit)/vaf_gjrgarch_fit@fit[["sigma"]]
```

```{r}
library(limma)

vaf_std_resids <- data.frame(vaf_std_resids)
rownames(vaf_std_resids) <- c()

hist(vaf_std_resids$vaf_std_resids)
qqt(vaf_std_resids, df=4.14, ylim=c(-6,6),xlim=c(-6,6))
qqline(vaf_std_resids,col = "steelblue", lwd = 2)




```

Definitely in improvement but still problems in the left tail, let's try the skewed student's-t distribution.

```{r}
vaf_returns <- vaf$LogReturn[2:nrow(vaf)]
daily_sd <- sqrt((vaf_returns-mean(vaf_returns))^2)
vaf_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"),distribution.model="sstd")

vaf_gjrgarch_fit <-ugarchfit(vaf_returns,spec=vaf_gjrgarch_spec)
vaf_std_resids <-residuals(vaf_gjrgarch_fit)/vaf_gjrgarch_fit@fit[["sigma"]]
```

```{r}
library(SkewHyperbolic)
vaf_std_resids <- data.frame(vaf_std_resids)
rownames(vaf_std_resids) <- c()

skewhypFit(vaf_std_resids$vaf_std_resids)

hist(vaf_std_resids$vaf_std_resids)
qqskewhyp(vaf_std_resids$vaf_std_resids, mu=0.1620,delta=1.4409,beta=-0.1724, nu=3.9620)

```

QQplot looks very good if we use MLE estimates for the Skew Student's-t distribution, however, they do not seem to correspond to the GARCH model estimates. Let's test with simulation.

```{r}
sim = ugarchsim(vaf_gjrgarch_fit,n.sim=2500)
sim_std_resids = sim@simulation[["residSim"]]/sim@simulation[["sigmaSim"]]
skewhypFit(sim_std_resids)
```

Estimated parameters from simulated model seem reasonably consistent with empirical values. Let's go with the skewed Student's-t distribution for the errors.

But, what does the shape (4.273049) and skew (0.887297) parameter correspond to in the estimation output?

In "Introduction to the rugarch package" on page 15, we may have a clue. It states that the package performs estimation using the $(\zeta,\rho)$ parameterisation and then transforms them into the $(\alpha, \beta, \delta, \mu)$ parameterisation.

```{r}

ghyptransform(mu=0,sigma=1,shape=4.273049,skew=0.887297)
```

The transformed parameters do not seem consistent with the ML estimates. Even though the skewed student's-t distribution is a special case of the generalised hyperbolic distribution, it doesn't seem that the ghyptransform can be directly applied in this case. I'm not going to try reconciling it any further so we'll try the ghyp distribution instead.

```{r}
vaf_returns <- vaf$LogReturn[2:nrow(vaf)]
daily_sd <- sqrt((vaf_returns-mean(vaf_returns))^2)
vaf_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"),distribution.model="ghyp")

vaf_gjrgarch_fit <-ugarchfit(vaf_returns,spec=vaf_gjrgarch_spec)
vaf_std_resids <-residuals(vaf_gjrgarch_fit)/vaf_gjrgarch_fit@fit[["sigma"]]
```

```{r}
library(ghyp)
vaf_std_resids <- data.frame(vaf_std_resids)
rownames(vaf_std_resids) <- c()

s_resids <-vaf_std_resids$vaf_std_resids
params <- ghyptransform(mu=mean(s_resids),sigma=sd(s_resids),shape=0.440242,skew=-0.453833,lambda=-1.937147)

s_resids_ghyp <- ghyp.ad(lambda=-1.937147,alpha=params[1,4],delta=params[1,2],beta=params[1,3],mu=params[1,1])

qqghyp(s_resids_ghyp,s_resids,gaussian=FALSE)
```

Does a better job of capturing the two most extreme values compared to the skewed student's-t, but for the less extreme values in the left tail, not as good. Let's move on to the next series.


##VAP

```{r}
vap_returns <- vap$LogReturn[2:nrow(vap)]
vap_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"))

vap_gjrgarch_fit <-ugarchfit(vap_returns,spec=vap_gjrgarch_spec)
vap_std_resids <-residuals(vap_gjrgarch_fit)/vap_gjrgarch_fit@fit[["sigma"]]
```

```{r}
vap_std_resids <- data.frame(vap_std_resids)
rownames(vap_std_resids) <- c()
hist(vap_std_resids$vap_std_resids)
qqnorm(vap_std_resids$vap_std_resids)
qqline(vap_std_resids$vap_std_resids, col = "steelblue", lwd = 2)

shapiro.test(vap_std_resids$vap_std_resids)

```

QQPlot looks pretty good, but Shapiro-Wilks test still strongly rejects normality. Let's try a student's t distribution.

```{r}

vap_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"),distribution.model="std")

vap_gjrgarch_fit <-ugarchfit(vap_returns,spec=vap_gjrgarch_spec)
vap_std_resids <-residuals(vap_gjrgarch_fit)/vap_gjrgarch_fit@fit[["sigma"]]
```

```{r}

vap_std_resids <- data.frame(vap_std_resids)
rownames(vap_std_resids) <- c()

hist(vap_std_resids$vap_std_resids)
qqt(vap_std_resids$vap_std_resids,df=7.743)
qqline(vap_std_resids$vap_std_resids, col = "steelblue", lwd = 2)


shapiro.test(vap_std_resids$vap_std_resids)

```

Student's t distribution seems to be an even better fit, judging by the QQPlot. One extreme value though in the left tail and underdispersion in the right tail. This seems to be good enough.

##VAS
```{r}
vas_returns <- vas$LogReturn[2:nrow(vas)]
vas_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"))

vas_gjrgarch_fit <-ugarchfit(vas_returns,spec=vas_gjrgarch_spec)
vas_std_resids <-residuals(vas_gjrgarch_fit)/vas_gjrgarch_fit@fit[["sigma"]]
```

```{r}
vas_std_resids <- data.frame(vas_std_resids)
rownames(vas_std_resids) <- c()

hist(vas_std_resids$vas_std_resids)
qqnorm(vas_std_resids$vas_std_resids)
qqline(vas_std_resids, col = "steelblue", lwd = 2)

shapiro.test(vas_std_resids$vas_std_resids)
```

Normality is in strong violation, trying student's-t distribution.

```{r}
vas_returns <- vas$LogReturn[2:nrow(vas)]
vas_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"),distribution.model="std")

vas_gjrgarch_fit <-ugarchfit(vas_returns,spec=vas_gjrgarch_spec)
vas_std_resids <-residuals(vas_gjrgarch_fit)/vas_gjrgarch_fit@fit[["sigma"]]
```

```{r}
vas_std_resids <- data.frame(vas_std_resids)
rownames(vas_std_resids) <- c()

hist(vas_std_resids$vas_std_resids)
qqt(vas_std_resids$vas_std_resids, df=9.533537)
qqline(vas_std_resids, col = "steelblue", lwd = 2)

```

Underdispersion in the right tail, let's use the ghyp distribution to model for asymmetry. 


```{r}
vas_returns <- vas$LogReturn[2:nrow(vas)]
daily_sd <- sqrt((vas_returns-mean(vas_returns))^2)
vas_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"),distribution.model="ghyp")

vas_gjrgarch_fit <-ugarchfit(vas_returns,spec=vas_gjrgarch_spec)
vas_std_resids <-residuals(vas_gjrgarch_fit)/vas_gjrgarch_fit@fit[["sigma"]]
```

```{r}
vas_std_resids <- data.frame(vas_std_resids)
rownames(vas_std_resids) <- c()

s_resids <-vas_std_resids$vas_std_resids
params <- ghyptransform(mu=mean(s_resids),sigma=sd(s_resids),shape=4.011653,skew=-0.323776,lambda=-1.734349)

s_resids_ghyp <- ghyp.ad(lambda=-1.734349,alpha=params[1,4],delta=params[1,2],beta=params[1,3],mu=params[1,1])

qqghyp(s_resids_ghyp,s_resids,gaussian=FALSE)
```

Superb fit. 

##VGS

```{r}
vgs_returns <- vgs$LogReturn[2:nrow(vgs)]
vgs_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"))

vgs_gjrgarch_fit <-ugarchfit(vgs_returns,spec=vgs_gjrgarch_spec)
vgs_std_resids <-residuals(vgs_gjrgarch_fit)/vgs_gjrgarch_fit@fit[["sigma"]]
```

```{r}
vgs_std_resids <- data.frame(vgs_std_resids)
rownames(vgs_std_resids) <- c()

hist(vgs_std_resids$vgs_std_resids)
qqnorm(vgs_std_resids$vgs_std_resids)
qqline(vgs_std_resids, col = "steelblue", lwd = 2)

shapiro.test(vgs_std_resids$vgs_std_resids)
```

Strong violation of normality. Heavy left tail. Unlikely that the student's t distribution will be an adequate fit, but let's try anyway.

```{r}
vgs_returns <- vgs$LogReturn[2:nrow(vgs)]
vgs_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"), distribution.model="std")

vgs_gjrgarch_fit <-ugarchfit(vgs_returns,spec=vgs_gjrgarch_spec)
vgs_std_resids <-residuals(vgs_gjrgarch_fit)/vgs_gjrgarch_fit@fit[["sigma"]]
```

```{r}
vgs_std_resids <- data.frame(vgs_std_resids)
rownames(vgs_std_resids) <- c()

hist(vgs_std_resids$vgs_std_resids)
qqt(vgs_std_resids$vgs_std_resids,df=5.849125)
qqline(vgs_std_resids, col = "steelblue", lwd = 2)

```

As expected, an inadequate fit, let's use the ghyp distribution.

```{r}
vgs_returns <- vgs$LogReturn[2:nrow(vgs)]
vgs_gjrgarch_spec <- ugarchspec(variance.model=list(model="gjrGARCH"), distribution.model="ghyp")

vgs_gjrgarch_fit <-ugarchfit(vgs_returns,spec=vgs_gjrgarch_spec)
vgs_std_resids <-residuals(vgs_gjrgarch_fit)/vgs_gjrgarch_fit@fit[["sigma"]]
```

```{r}
vgs_std_resids <- data.frame(vgs_std_resids)
rownames(vgs_std_resids) <- c()
s_resids <-vas_std_resids$vas_std_resids

params <- ghyptransform(mu=mean(s_resids),sigma=sd(s_resids),shape=0.250194,skew=-0.921569,lambda=-3.333675)

s_resids_ghyp <- ghyp.ad(lambda=-3.333675,alpha=params[1,4],delta=params[1,2],beta=params[1,3],mu=params[1,1])

qqghyp(s_resids_ghyp,s_resids,gaussian=FALSE)

```

Mostly underdispersed, but a slightly better fit than the student's t distribution.

